

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Detection &mdash; BiaPy  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Denoising" href="denoising.html" />
    <link rel="prev" title="Instance segmentation" href="instance_segmentation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> BiaPy
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/how_it_works.html">How it works</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/select_workflow.html">Select workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Workflows</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="semantic_segmentation.html">Semantic segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="instance_segmentation.html">Instance segmentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Detection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-preparation">Data preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#problem-resolution">Problem resolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-file">Configuration file</a></li>
<li class="toctree-l2"><a class="reference internal" href="#special-workflow-configuration">Special workflow configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run">Run</a></li>
<li class="toctree-l2"><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="denoising.html">Denoising</a></li>
<li class="toctree-l1"><a class="reference internal" href="super_resolution.html">Super-resolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="self_supervision.html">Self-supervision</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">Classification</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/stable.html">Stable DNN architectures for mitochondria segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/cartocell.html">CartoCell, a high-throughput pipeline for accurate 3D image analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/mitoem.html">MitoEM dataset: large-scale 3d mitochondria instance segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/nucleus.html">NucMM dataset: 3d neuronal nuclei instance segmentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../config/main_config.html">config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data/data.html">data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../engine/engine.html">engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/models.html">models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/utils.html">utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Bibliography</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">BiaPy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Detection</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="detection">
<span id="id1"></span><h1>Detection<a class="headerlink" href="#detection" title="Permalink to this headline">¶</a></h1>
<p>The goal of this workflow is to localize objects in the input image, not requiring a pixel-level class. Common strategies produce either bounding boxes containing the objects or individual points at their center of mass <span id="id2">[<a class="reference internal" href="../bibliography.html#id7" title="X. Zhou, D. Wang, and P. Krähenbühl. Objects as points. arXiv preprint arXiv:1904.07850, 2019.">ZWKrahenbuhl19</a>]</span>, which is the one adopted by BiaPy.</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>Input:</strong></dt><dd><ul>
<li><p>Image.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.csv</span></code> file containing the list of points to detect.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Output:</strong></dt><dd><ul>
<li><p>Image with the detected points as white dots.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.csv</span></code> file with the list of detected points in napari format.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">_prob.csv</span></code> file with the same list of points as above but now with their detection probability (also in napary format).</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>In the figure below an example of this workflow’s <strong>input</strong> is depicted:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><div class="figure align-center" id="id3">
<img alt="../_images/detection_image_input.png" src="../_images/detection_image_input.png" />
<p class="caption"><span class="caption-text">Input image.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
</td>
<td><div class="figure align-center" id="id4">
<img alt="../_images/detection_csv_input.svg" src="../_images/detection_csv_input.svg" /><p class="caption"><span class="caption-text">Input <code class="docutils literal notranslate"><span class="pre">.csv</span></code> file.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
</td>
</tr>
</tbody>
</table>
<p>Description of the <code class="docutils literal notranslate"><span class="pre">.csv</span></code> file:</p>
<blockquote>
<div><ul class="simple">
<li><p>Each row represents the middle point of the object to be detected. Each column is a coordinate in the image dimension space.</p></li>
<li><p>The first column name does not matter but it needs to be there. No matter also the enumeration and order for that column.</p></li>
<li><p>If the images are <code class="docutils literal notranslate"><span class="pre">3D</span></code>, three columns need to be present and their names must be <code class="docutils literal notranslate"><span class="pre">[axis-0,</span> <span class="pre">axis-1,</span> <span class="pre">axis-2]</span></code>, which represent <code class="docutils literal notranslate"><span class="pre">(z,y,x)</span></code> axes. If the images are <code class="docutils literal notranslate"><span class="pre">2D</span></code>, only two columns are required <code class="docutils literal notranslate"><span class="pre">[axis-0,</span> <span class="pre">axis-1]</span></code>, which represent <code class="docutils literal notranslate"><span class="pre">(y,x)</span></code> axes.</p></li>
<li><p>For multi-class detection problem, i.e. <code class="docutils literal notranslate"><span class="pre">MODEL.N_CLASSES</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, add an additional <code class="docutils literal notranslate"><span class="pre">class</span></code> column to the file. The classes need to start from <code class="docutils literal notranslate"><span class="pre">1</span></code> and consecutive, i.e. <code class="docutils literal notranslate"><span class="pre">1,2,3,4...</span></code> and not like <code class="docutils literal notranslate"><span class="pre">1,4,8,6...</span></code>.</p></li>
<li><p>Coordinates can be float or int but they will be converted into ints so they can be translated to pixels.</p></li>
</ul>
</div></blockquote>
<div class="section" id="data-preparation">
<span id="detection-data-prep"></span><h2>Data preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h2>
<p>To ensure the proper operation of the library the data directory tree should be something like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>dataset/
├── train
│   ├── x
│   │   ├── training-0001.tif
│   │   ├── training-0002.tif
│   │   ├── . . .
│   │   ├── training-9999.tif
│   └── y
│       ├── training_groundtruth-0001.csv
│       ├── training_groundtruth-0002.csv
│       ├── . . .
│       ├── training_groundtruth-9999.csv
└── test
    ├── x
    │   ├── testing-0001.tif
    │   ├── testing-0002.tif
    │   ├── . . .
    │   ├── testing-9999.tif
    └── y
        ├── testing_groundtruth-0001.csv
        ├── testing_groundtruth-0002.csv
        ├── . . .
        ├── testing_groundtruth-9999.csv
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Ensure that images and their corresponding masks are sorted in the same way. A common approach is to fill with zeros the image number added to the filenames (as in the example).</p>
</div>
</div>
<div class="section" id="problem-resolution">
<span id="detection-problem-resolution"></span><h2>Problem resolution<a class="headerlink" href="#problem-resolution" title="Permalink to this headline">¶</a></h2>
<p>Firstly, a <strong>pre-processing</strong> step is done where the list of points of the <code class="docutils literal notranslate"><span class="pre">.csv</span></code> file is transformed into point mask images. During this process some checks are made to ensure there is not repeated point in the <code class="docutils literal notranslate"><span class="pre">.csv</span></code>. This option is enabled by default with <code class="docutils literal notranslate"><span class="pre">PROBLEM.DETECTION.CHECK_POINTS_CREATED</span></code> so if any problem is found the point mask of that <code class="docutils literal notranslate"><span class="pre">.csv</span></code> will not be created until the problem is solve.</p>
<p>After the train phase, the model output will be an image where each pixel of each channel will have the probability (in <code class="docutils literal notranslate"><span class="pre">[0-1]</span></code> range) of being of the class that represents that channel. The image would be something similar to the left picture below:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><div class="figure align-center" id="id5">
<img alt="../_images/detection_probs.png" src="../_images/detection_probs.png" />
<p class="caption"><span class="caption-text">Model output.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
</td>
<td><div class="figure align-center" id="id6">
<img alt="../_images/detected_points.png" src="../_images/detected_points.png" />
<p class="caption"><span class="caption-text">Final points considered.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
</td>
</tr>
</tbody>
</table>
<p>So those probability images, as the left picture above, can be converted into the final points, as the rigth picture above, we use <a class="reference external" href="https://scikit-image.org/docs/stable/api/skimage.feature.html#peak-local-max">peak_local_max</a> function to find peaks in those probability clouds. For that, you need to define a threshold, <code class="docutils literal notranslate"><span class="pre">TEST.DET_MIN_TH_TO_BE_PEAK</span></code> variable in our case, for the minimum probability to be considered as a point. You can set different thresholds for each class in <code class="docutils literal notranslate"><span class="pre">TEST.DET_MIN_TH_TO_BE_PEAK</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">[0.7,0.9]</span></code>.</p>
<p>After this process you can apply a <strong>post-processing</strong> to remove possible close points with <code class="docutils literal notranslate"><span class="pre">TEST.POST_PROCESSING.REMOVE_CLOSE_POINTS</span></code>. For that you need to define a radius to remove the point around each one with <code class="docutils literal notranslate"><span class="pre">TEST.POST_PROCESSING.REMOVE_CLOSE_POINTS_RADIUS</span></code> variable. You can set different radius for each class, e.g. <code class="docutils literal notranslate"><span class="pre">[0.7,0.9]</span></code>. In this post-processing is important to set <code class="docutils literal notranslate"><span class="pre">DATA.TEST.RESOLUTION</span></code>, specially for <code class="docutils literal notranslate"><span class="pre">3D</span></code> data where the resolution in <code class="docutils literal notranslate"><span class="pre">z</span></code> dimension is usually less than in other axes. That resolution will be taken into account when removing points.</p>
<p>Finally, the output files are generated from the remaining final points.</p>
</div>
<div class="section" id="configuration-file">
<h2>Configuration file<a class="headerlink" href="#configuration-file" title="Permalink to this headline">¶</a></h2>
<p>Find in <a class="reference external" href="https://github.com/danifranco/BiaPy/tree/master/templates/detection">templates/detection</a> folder of BiaPy a few YAML configuration templates for this workflow.</p>
</div>
<div class="section" id="special-workflow-configuration">
<h2>Special workflow configuration<a class="headerlink" href="#special-workflow-configuration" title="Permalink to this headline">¶</a></h2>
<p>Here some special configuration options that can be selected in this workflow are described:</p>
<ul>
<li><p><strong>Metrics</strong>: during the inference phase the performance of the test data is measured using different metrics if test masks were provided (i.e. ground truth) and, consequently, <code class="docutils literal notranslate"><span class="pre">DATA.TEST.LOAD_GT</span></code> is enabled. In the case of detection the <strong>Intersection over Union</strong> (IoU), precision, recall and F1 are calculated:</p>
<ul class="simple">
<li><p><strong>IoU</strong> metric, also referred as the Jaccard index, is essentially a method to quantify the percent of overlap between the target mask and the prediction output. Depending on the configuration different values are calculated (as explained in <a class="reference internal" href="../get_started/configuration.html#config-test"><span class="std std-ref">Test phase</span></a>).</p></li>
<li><p><strong>Precision</strong>, is the fraction of relevant instances among the retrieved instances. More info <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">here</a>.</p></li>
<li><p><strong>Recall</strong>, is the fraction of relevant instances that were retrieved. More info <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">here</a>.</p></li>
<li><p><strong>F1</strong>, is the harmonic mean of the precision and recall. More info <a class="reference external" href="https://en.wikipedia.org/wiki/F-score">here</a>.</p></li>
</ul>
<p>The last three metrics, i.e. precision, recall and F1, use <code class="docutils literal notranslate"><span class="pre">TEST.DET_TOLERANCE</span></code> to determine when a point is considered as a true positive. In this process the test resolution is also taken into account. You can set different tolerances for each class, e.g. <code class="docutils literal notranslate"><span class="pre">[10,15]</span></code>.</p>
</li>
<li><p><strong>Post-processing</strong>: you an use <code class="docutils literal notranslate"><span class="pre">TEST.POST_PROCESSING.REMOVE_CLOSE_POINTS</span></code> to remove redundant close points to each other as described previously in <a class="reference internal" href="#detection-problem-resolution"><span class="std std-ref">Problem resolution</span></a>.</p></li>
</ul>
</div>
<div class="section" id="run">
<h2>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h2>
<p><strong>Command line</strong>: Open a terminal as described in <a class="reference internal" href="../get_started/installation.html#installation"><span class="std std-ref">Installation</span></a>. For instance, using <a class="reference external" href="https://github.com/danifranco/BiaPy/blob/master/templates/detection/unet_3d_detection.yaml">unet_3d_detection.yaml</a> template file, the code can be run as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configuration file</span>
<span class="nv">job_cfg_file</span><span class="o">=</span>/home/user/unet_3d_detection.yaml
<span class="c1"># Where the experiment output directory should be created</span>
<span class="nv">result_dir</span><span class="o">=</span>/home/user/exp_results
<span class="c1"># Just a name for the job</span>
<span class="nv">job_name</span><span class="o">=</span>unet_detection_3d
<span class="c1"># Number that should be increased when one need to run the same job multiple times (reproducibility)</span>
<span class="nv">job_counter</span><span class="o">=</span><span class="m">1</span>
<span class="c1"># Number of the GPU to run the job in (according to &#39;nvidia-smi&#39; command)</span>
<span class="nv">gpu_number</span><span class="o">=</span><span class="m">0</span>

<span class="c1"># Move where BiaPy installation resides</span>
<span class="nb">cd</span> BiaPy

<span class="c1"># Load the environment</span>
conda activate BiaPy_env

python -u main.py <span class="se">\</span>
       --config <span class="nv">$job_cfg_file</span> <span class="se">\</span>
       --result_dir <span class="nv">$result_dir</span>  <span class="se">\</span>
       --name <span class="nv">$job_name</span>    <span class="se">\</span>
       --run_id <span class="nv">$job_counter</span>  <span class="se">\</span>
       --gpu <span class="nv">$gpu_number</span>
</pre></div>
</div>
<p><strong>Docker</strong>: Open a terminal as described in <a class="reference internal" href="../get_started/installation.html#installation"><span class="std std-ref">Installation</span></a>. For instance, using <a class="reference external" href="https://github.com/danifranco/BiaPy/blob/master/templates/detection/unet_3d_detection.yaml">unet_3d_detection.yaml</a> template file, the code can be run as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configuration file</span>
<span class="nv">job_cfg_file</span><span class="o">=</span>/home/user/unet_3d_detection.yaml
<span class="c1"># Where the experiment output directory should be created</span>
<span class="nv">result_dir</span><span class="o">=</span>/home/user/exp_results
<span class="c1"># Just a name for the job</span>
<span class="nv">job_name</span><span class="o">=</span>unet_detection_3d
<span class="c1"># Number that should be increased when one need to run the same job multiple times (reproducibility)</span>
<span class="nv">job_counter</span><span class="o">=</span><span class="m">1</span>
<span class="c1"># Number of the GPU to run the job in (according to &#39;nvidia-smi&#39; command)</span>
<span class="nv">gpu_number</span><span class="o">=</span><span class="m">0</span>

docker run --rm <span class="se">\</span>
    --gpus <span class="nv">$gpu_number</span> <span class="se">\</span>
    --mount <span class="nv">type</span><span class="o">=</span>bind,source<span class="o">=</span><span class="nv">$job_cfg_file</span>,target<span class="o">=</span><span class="nv">$job_cfg_file</span> <span class="se">\</span>
    --mount <span class="nv">type</span><span class="o">=</span>bind,source<span class="o">=</span><span class="nv">$result_dir</span>,target<span class="o">=</span><span class="nv">$result_dir</span> <span class="se">\</span>
    --mount <span class="nv">type</span><span class="o">=</span>bind,source<span class="o">=</span><span class="nv">$data_dir</span>,target<span class="o">=</span><span class="nv">$data_dir</span> <span class="se">\</span>
    danifranco/em_image_segmentation <span class="se">\</span>
        -cfg <span class="nv">$job_cfg_file</span> <span class="se">\</span>
        -rdir <span class="nv">$result_dir</span> <span class="se">\</span>
        -name <span class="nv">$job_name</span> <span class="se">\</span>
        -rid <span class="nv">$job_counter</span> <span class="se">\</span>
        -gpu <span class="nv">$gpu_number</span>
</pre></div>
</div>
</div>
<div class="section" id="results">
<span id="detection-results"></span><h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<p>The results are placed in <code class="docutils literal notranslate"><span class="pre">results</span></code> folder under <code class="docutils literal notranslate"><span class="pre">--result_dir</span></code> directory with the <code class="docutils literal notranslate"><span class="pre">--name</span></code> given.</p>
<p>Following the example, you should see that the directory <code class="docutils literal notranslate"><span class="pre">/home/user/exp_results/unet_detection_3d</span></code> has been created. If the same experiment is run 5 times, varying <code class="docutils literal notranslate"><span class="pre">--run_id</span></code> argument only, you should find the following directory tree:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>unet_detection_3d/
├── config_files/
│   └── unet_3d_detection.yaml
├── checkpoints
│   └── model_weights_unet_detection_3d_1.h5
└── results
    ├── unet_detection_3d_1
    ├── . . .
    └── unet_detection_3d_5
        ├── cell_counter.csv
        ├── aug
        │   └── .tif files
        ├── charts
        │   ├── unet_detection_3d_1_jaccard_index.png
        │   ├── unet_detection_3d_1_loss.png
        │   └── model_plot_unet_detection_3d_1.png
        ├── per_image
        │   └── .tif files
        └── per_image_local_max_check
            └── .tif files
</pre></div>
</div>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">config_files</span></code>: directory where the .yaml filed used in the experiment is stored.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">unet_3d_detection.yaml</span></code>: YAML configuration file used (it will be overwrited every time the code is run).</p></li>
</ul>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">checkpoints</span></code>: directory where model’s weights are stored.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_weights_unet_detection_3d_1.h5</span></code>: model’s weights file.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">results</span></code>: directory where all the generated checks and results will be stored. There, one folder per each run are going to be placed.</p>
<blockquote>
<div><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">unet_detection_3d_1</span></code>: run 1 experiment folder.</p>
<blockquote>
<div><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">cell_counter.csv</span></code>: file with a counter of detected objects for each test sample.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">aug</span></code>: image augmentation samples.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">charts</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">unet_detection_3d_1_jaccard_index.png</span></code>: IoU (jaccard_index) over epochs plot (when training is done).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">unet_detection_3d_1_loss.png</span></code>: Loss over epochs plot (when training is done).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_plot_unet_detection_3d_1.png</span></code>: plot of the model.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">per_image</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">.tif</span> <span class="pre">files</span></code>: reconstructed images from patches.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">per_image_local_max_check</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">.tif</span> <span class="pre">files</span></code>: Same as <code class="docutils literal notranslate"><span class="pre">per_image</span></code> but with the final detected points.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Here, for visualization purposes, only <code class="docutils literal notranslate"><span class="pre">unet_detection_3d_1</span></code> has been described but <code class="docutils literal notranslate"><span class="pre">unet_detection_3d_2</span></code>, <code class="docutils literal notranslate"><span class="pre">unet_detection_3d_3</span></code>, <code class="docutils literal notranslate"><span class="pre">unet_detection_3d_4</span></code> and <code class="docutils literal notranslate"><span class="pre">unet_detection_3d_5</span></code> will follow the same structure.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="denoising.html" class="btn btn-neutral float-right" title="Denoising" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="instance_segmentation.html" class="btn btn-neutral float-left" title="Instance segmentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, Daniel Franco-Barranco.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>